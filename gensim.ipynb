{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from string import punctuation\n",
    "\n",
    "import numpy as np\n",
    "import spacy as spc\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.test.utils import datapath\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score, fowlkes_mallows_score, v_measure_score\n",
    "from umap import UMAP\n",
    "\n",
    "nlp = spc.load(\"en_core_web_sm\", disable=[\"ner\"])\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random_state = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "20\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(subset=\"all\", random_state=random_state, remove=(\"headers\", \"footers\", \"quoter\"))\n",
    "corpus = dataset.data\n",
    "y = dataset.target\n",
    "y_names = dataset.target_names\n",
    "print(len(np.unique(y_names)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "path_to_preprocessed_corpus = \"preprocessed_corpus.pkl\"\n",
    "if Path(path_to_preprocessed_corpus).is_file():\n",
    "    with open(path_to_preprocessed_corpus, \"rb\") as f:\n",
    "        preprocessed_corpus = pickle.load(f)\n",
    "else:\n",
    "    preprocessed_corpus = []\n",
    "    for i, text in enumerate(corpus):\n",
    "        doc = nlp(text.lower(), disable=[\"tagger\", \"parser\"])\n",
    "        text = \" \".join([\n",
    "            token.lemma_.strip(punctuation) for token in doc if not token.is_stop and not token.is_punct and\n",
    "                                                                not token.like_email and not token.is_bracket and\n",
    "                                                                not token.is_quote and not token.is_currency and \n",
    "                                                                not token.like_num and not token.is_space and\n",
    "                                                                not token.like_url and token.lemma_ != \"-PRON-\"\n",
    "        ])\n",
    "        doc = nlp(text)\n",
    "        preprocessed_corpus.append([\n",
    "            token.lemma_.strip() for token in doc if not token.is_stop and not token.is_punct and\n",
    "                                                     not token.like_email and not token.is_bracket and\n",
    "                                                     not token.is_quote and not token.is_currency and \n",
    "                                                     not token.like_num and not token.is_space and\n",
    "                                                     not token.like_url and token.lemma_.isalpha()\n",
    "        ])\n",
    "    with open(path_to_preprocessed_corpus, \"wb\") as f:\n",
    "        pickle.dump(preprocessed_corpus, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "0.036*\"game\" + 0.027*\"team\" + 0.018*\"play\" + 0.015*\"win\" + 0.012*\"hockey\"\n",
      "0.014*\"people\" + 0.011*\"government\" + 0.010*\"right\" + 0.010*\"think\" + 0.008*\"write\"\n",
      "0.023*\"file\" + 0.018*\"image\" + 0.018*\"window\" + 0.012*\"program\" + 0.010*\"use\"\n",
      "0.037*\"key\" + 0.016*\"chip\" + 0.013*\"use\" + 0.012*\"encryption\" + 0.009*\"phone\"\n",
      "0.019*\"wire\" + 0.015*\"use\" + 0.012*\"db\" + 0.012*\"food\" + 0.010*\"grind\"\n",
      "0.019*\"don\" + 0.018*\"v\" + 0.013*\"doug\" + 0.011*\"ld\" + 0.010*\"write\"\n",
      "0.017*\"car\" + 0.015*\"write\" + 0.012*\"like\" + 0.012*\"article\" + 0.009*\"look\"\n",
      "0.104*\"x\" + 0.024*\"t\" + 0.021*\"o\" + 0.021*\"p\" + 0.020*\"w\"\n",
      "0.067*\"bike\" + 0.037*\"ride\" + 0.023*\"motorcycle\" + 0.023*\"dog\" + 0.020*\"dod\"\n",
      "0.012*\"ham\" + 0.012*\"darren\" + 0.010*\"joy\" + 0.010*\"bh\" + 0.010*\"clark\"\n",
      "0.019*\"space\" + 0.011*\"write\" + 0.010*\"earth\" + 0.009*\"launch\" + 0.008*\"orbit\"\n",
      "0.037*\"gun\" + 0.016*\"weapon\" + 0.014*\"drug\" + 0.013*\"firearm\" + 0.011*\"crime\"\n",
      "0.020*\"god\" + 0.016*\"write\" + 0.014*\"people\" + 0.012*\"know\" + 0.012*\"think\"\n",
      "0.016*\"mail\" + 0.013*\"information\" + 0.012*\"post\" + 0.011*\"list\" + 0.010*\"send\"\n",
      "0.010*\"armenian\" + 0.010*\"war\" + 0.010*\"people\" + 0.009*\"armenians\" + 0.009*\"turkish\"\n",
      "0.018*\"year\" + 0.016*\"write\" + 0.013*\"article\" + 0.011*\"think\" + 0.010*\"run\"\n",
      "0.031*\"israel\" + 0.028*\"jews\" + 0.018*\"jewish\" + 0.018*\"israeli\" + 0.013*\"arab\"\n",
      "0.012*\"report\" + 0.011*\"university\" + 0.011*\"new\" + 0.011*\"health\" + 0.009*\"american\"\n",
      "0.016*\"drive\" + 0.013*\"card\" + 0.010*\"system\" + 0.010*\"work\" + 0.009*\"disk\"\n",
      "0.008*\"evidence\" + 0.008*\"man\" + 0.007*\"write\" + 0.007*\"book\" + 0.006*\"argument\"\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "path = datapath(\"model\")\n",
    "dictionary = Dictionary(preprocessed_corpus)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_corpus]\n",
    "\n",
    "if not Path(path).exists():\n",
    "    lda = LdaModel(bow_corpus, num_topics=20, id2word=dictionary, passes=10)\n",
    "    lda.save(path)\n",
    "else:\n",
    "    lda = LdaModel.load(path)\n",
    "\n",
    "\n",
    "for topic in lda.print_topics(num_words=5):\n",
    "    print(topic[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(18846, 20)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "embedding = []\n",
    "for topics in lda.get_document_topics(bow_corpus, minimum_probability=0):\n",
    "    embedding.append([topic[1] for topic in sorted(topics, key=lambda x: x[0])])\n",
    "embedding = np.array(embedding)\n",
    "# for row in embedding:\n",
    "#     print(row)\n",
    "print(embedding.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c59fa388a2f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m embedding_2d = UMAP(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_dist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m ).fit_transform(embedding)\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thalvari\\pycharmprojects\\nlp20_project\\venv\\lib\\site-packages\\umap\\umap_.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1596\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1597\u001b[0m         \"\"\"\n\u001b[1;32m-> 1598\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1599\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thalvari\\pycharmprojects\\nlp20_project\\venv\\lib\\site-packages\\umap\\umap_.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1340\u001b[0m         \"\"\"\n\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1342\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1343\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raw_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thalvari\\pycharmprojects\\nlp20_project\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32mc:\\users\\thalvari\\pycharmprojects\\nlp20_project\\venv\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ],
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error"
    }
   ],
   "source": [
    "embedding_2d = UMAP(\n",
    "    n_components=2, n_neighbors=30, min_dist=0.0, random_state=random_state\n",
    ").fit_transform(embedding)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "clusters: 20\n",
      "cluster sizes: [(13, 2751), (0, 2628), (5, 1704), (4, 1167), (17, 1139), (19, 1084), (18, 899), (1, 875), (8, 827), (9, 803), (6, 746), (3, 684), (10, 580), (12, 557), (2, 537), (7, 522), (14, 507), (16, 386), (15, 294), (11, 156)]\n",
      "noise level: 0.0\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# clusters = HDBSCAN(\n",
    "#     min_samples=1, min_cluster_size=150, core_dist_n_jobs=-1\n",
    "# ).fit_predict(embedding_2d)\n",
    "\n",
    "clusters = SpectralClustering(\n",
    "    n_clusters=20, random_state=random_state, n_jobs=-1\n",
    ").fit_predict(embedding)\n",
    "\n",
    "counter = Counter(clusters)\n",
    "print(f\"clusters: {np.amax(clusters) + 1}\")\n",
    "print(f\"cluster sizes: {sorted(counter.items(), key=lambda x: x[1], reverse=True)}\")\n",
    "print(f\"noise level: {np.round(counter[-1] / len(clusters), 3)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "AMI: 0.39531564909483047\n",
      "ARI: 0.18213953356265494\n",
      "V-measure: 0.3973362443082913\n",
      "Fowlkes-Mallows: 0.23637860513954634\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(f\"AMI: {adjusted_mutual_info_score(y , clusters)}\")\n",
    "print(f\"ARI: {adjusted_rand_score(y , clusters)}\")\n",
    "print(f\"V-measure: {v_measure_score(y , clusters)}\")\n",
    "print(f\"Fowlkes-Mallows: {fowlkes_mallows_score(y , clusters)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}